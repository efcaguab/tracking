---
title: "Peskas: tracking domain"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This repository contains code for the tracking data pipeline.

## Pipeline description

*Pipeline input(s)*: Tracking data obtained from the Pelagic Data Systems API. 

*Pipeline product(s)*: A BigQuery dataset with clean catch data. The dataset includes tables with *(i)* the full clean dataset.

**Stages**:

1. Input data is retrieved on a daily basis at midnight (UTM) by a Google Build Service from the Pelagic Data Systems restful API. Daily tracks are stored in a Google Storage bucket.
1. Changes in the storage file are automatically detected and trigger a Google pub/sub notification.
1. The pub/sub message triggers a call to an API processing function that reads the data from the storage. The function then validates, cleans, and transforms, the data. Finally, the clean data is deposited in a Google BigQuery table. The processing function runs serverless in Google CloudRun.

## Deployment steps

- [Create a service account](https://cloud.google.com/iam/docs/creating-managing-service-accounts) for this domain: [`deployment/create-service-account.sh`](deployment/create-service-account.sh)
- [Create a key](https://cloud.google.com/iam/docs/creating-managing-service-account-keys) for the service account: [`deployment/create-service-account-key.sh`](deployment/create-service-account-key.sh)
- [Upload the key](https://cloud.google.com/secret-manager/docs/creating-and-accessing-secrets#secretmanager-create-secret-cli) to Google Secret Manager to simplify reuse: [`deployment/create-secret.sh`](deployment/create-secret.sh)
- [Create the storage](https://cloud.google.com/storage/docs/creating-buckets) bucket: [`deployment/create-storage-bucket.sh`](deployment/create-storage-bucket.sh)
